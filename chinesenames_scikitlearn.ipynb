{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline   # pipeline of transforms with a final estimator\n",
    "from sklearn.model_selection import GridSearchCV  # search over parameter values for an estimator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Normalizer, LabelEncoder\n",
    "import json\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t_start = time.time()\n",
    "        res = func(*args, **kwargs)\n",
    "        print(\"{} # elapsed time: {:.0f} m {:.0f}s\".format(\n",
    "            func.__name__.upper(), *divmod(time.time() - t_start, 60)))\n",
    "        return res\n",
    "    return wrapper\n",
    "\n",
    "class WordLengths(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"extract word lengths from a full name\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, d):\n",
    "        # d is a 1-column data frame with full names\n",
    "        return(pd.concat([d.apply(lambda x: len(str(x).split()[0])), \n",
    "                          d.apply(lambda x: len(str(x).split()[-1])),\n",
    "                          d.str.len(),\n",
    "                          d.apply(lambda x: len(str(x).split())),\n",
    "                          d.apply(lambda x: sum(len(w) > 1 for w in str(x).split()))\n",
    "                         ], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseNameDetector(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        read up data into a data frame that look like below             \n",
    "\n",
    "                                 full_name  is_chinese\n",
    "        0      dianne  van eck           0\n",
    "        1         chen zaichun           1\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv('~/Data/names/chinesenames-data.csv')\n",
    "        print(\"name dataset contains {} names ({} chinese)\".format(len(self.data), Counter(self.data.is_chinese)[1]))\n",
    "\n",
    "        assert set(self.data.columns) == set({\"full_name\", \"is_chinese\"}), print(\"wrong column names in data csv...\")\n",
    "        assert sum(list(Counter(self.data.is_chinese).values())) == len(self.data), print(\n",
    "            \"seems like not all names in data are labelled...\")\n",
    "        # add hypocorisms\n",
    "        \n",
    "        hyps = json.load(open('/Users/ik/Data/names/hypocorisms.json', 'r'))\n",
    "        self.hyps_set = {h for name in hyps for h in hyps[name]}\n",
    "        \n",
    "        # exclude hypocorisms\n",
    "#         self.data['full_name'] = self.data[\"full_name\"].apply(lambda x: ' '.join([w for w in x.split() if w not in self.hyps_set])).str.strip()\n",
    "#         self.data = self.data[self.data.full_name.str.len() > 1]\n",
    "\n",
    "    @timer\n",
    "    def train_model(self):\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "                            ('features', FeatureUnion([\n",
    "                                    ('lengths', WordLengths()),\n",
    "                                ('vectw', CountVectorizer(analyzer='word', ngram_range=(1,1))),\n",
    "                                    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,3)))])),\n",
    "            #  -- regularized linear model with stochastic gradient descent (SGD) learning\n",
    "            #  max_iter = number of epochs\n",
    "            ('normalize', Normalizer()),\n",
    "            ('clf', SGDClassifier(max_iter=100, learning_rate='optimal'))])  \n",
    "        \n",
    "        parameters = {'features__vect__ngram_range': ((1,3), (1,4), (1,5)),  # unigrams or bigrams\n",
    "                        'clf__alpha': (0.00001, 0.000001),\n",
    "                        'clf__penalty': ('l2', 'elasticnet')}\n",
    "        \n",
    "        self.grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.data['full_name'], \n",
    "                                                            self.data['is_chinese'], \n",
    "                                                            stratify=self.data['is_chinese'],\n",
    "                                                            test_size=0.25, random_state=31)\n",
    "        \n",
    "        self.grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"best score: {:.2f}\".format(self.grid_search.best_score_))\n",
    "        print(\"best parameters: {}\".format(self.grid_search.best_params_))\n",
    "        y_true, y_pred = y_test, self.grid_search.predict(X_test)\n",
    "        X_test[y_true - y_pred == 1].to_csv('true_ch_pred_no.csv', index=False)\n",
    "        X_test[y_pred - y_true == 1].to_csv('no_ch_pred_y.csv', index=False)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name dataset contains 687591 names (240351 chinese)\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 13.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 1.00\n",
      "best parameters: {'clf__alpha': 1e-06, 'clf__penalty': 'l2', 'features__vect__ngram_range': (1, 5)}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    111810\n",
      "          1       0.99      0.99      0.99     60088\n",
      "\n",
      "avg / total       1.00      1.00      1.00    171898\n",
      "\n",
      "[[111501    309]\n",
      " [   302  59786]]\n",
      "TRAIN_MODEL # elapsed time: 14 m 9s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    cnd = ChineseNameDetector()\n",
    "    cnd.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnd.grid_search.predict(pd.Series(['su willians']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
